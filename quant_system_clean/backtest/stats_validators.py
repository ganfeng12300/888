# -*- coding: utf-8 -*-
import numpy as np, pandas as pd
from scipy.stats import norm
from statsmodels.tsa.stattools import acf

def equity_metrics(eq: pd.Series):
    if len(eq)<2: return {"æ€»æ”¶ç›?%)":np.nan,"å¹´åŒ–(%)":np.nan,"å¤æ™®æ¯?:np.nan,"æœ€å¤§å›žæ’?%)":np.nan}
    ret=eq.pct_change().fillna(0.0)
    sharpe=np.sqrt(365)*ret.mean()/(ret.std()+1e-12)
    peak=eq.cummax(); dd=(eq/peak-1.0).min()
    days=max(1,len(eq)/24.0); cagr=(eq.iloc[-1]/eq.iloc[0])**(365/days)-1.0
    return {"æ€»æ”¶ç›?%)":100*float(eq.iloc[-1]-1.0), "å¹´åŒ–(%)":100*float(cagr),
            "å¤æ™®æ¯?:float(sharpe), "æœ€å¤§å›žæ’?%)":100*float(-dd)}

def walk_forward_splits(n, k=5):
    # å‡åŒ€åˆ‡ä¸º k æŠ˜ï¼ˆç®€å•ç‰ˆï¼?
    b=np.linspace(0,n,k+1, dtype=int)
    return [(b[i], b[i+1]) for i in range(k)]

def deflated_sharpe(sharpe, n_strats, n_obs):
    # Bailey & LÃ³pez de Prado è¿‘ä¼¼ï¼ˆç®€åŒ–ï¼‰
    if n_obs<=1: return np.nan
    emax = (1-0.75*np.log(np.log(n_strats))-0.5/np.log(n_strats)) if n_strats>1 else 0
    return float(max(0.0, sharpe - emax*np.sqrt((1-0.01)/max(1,n_obs))))

def spa_significance(scores: np.ndarray, B=500):
    # SPA ç®€åŒ–ï¼šç½®æ¢é‡é‡‡æ ·åˆ¤æ–­æœ€ä½³æ˜¯å¦æ˜¾è‘—ä¼˜äºŽé›¶åŸºå‡†
    best=scores.max()
    cnt=0
    for _ in range(B):
        perm=np.random.permutation(scores)
        if perm.max()>=best: cnt+=1
    p=cnt/max(1,B)
    return p<0.05, p

def probability_of_backtest_overfitting(ranks_in, ranks_out, bins=10):
    # PBOï¼šIn-sample æŽ’åä¸?OOS æŽ’åçš?Kendall-like ååºç¨‹åº¦
    if len(ranks_in)!=len(ranks_out) or len(ranks_in)==0:
        return np.nan
    x=pd.Series(ranks_in).rank(pct=True).values
    y=pd.Series(ranks_out).rank(pct=True).values
    # è®¡ç®—ååºæ¦‚çŽ‡ï¼ˆç®€åŒ–ï¼‰
    inv=np.mean((x<0.5)&(y>0.5))
    return float(inv)
